{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c35af1-f84a-412d-a59f-dedaf1cc2460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import time \n",
    "import os\n",
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f70ce24-e85a-469b-8913-5a49635c57ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For calculating statistical northness and eastness\n",
    "\n",
    "filename_physstates_2d = '/cluster/projects/itk-SINMOD/coral-mapping/midnor/samp_2D_jan_jun.nc'\n",
    "physstates_2d = Dataset(filename_physstates_2d, 'r')\n",
    "gridLons = physstates_2d.variables['gridLons']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48fb4625-cfb2-4c82-a8ca-37acd3fba5cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<string>:21\u001b[0;36m\u001b[0m\n\u001b[0;31m    if output_path is not None:\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "#Create bottom-features\n",
    "\n",
    "def process_bottom_layer(filepath,\n",
    "    variable_name,\n",
    "    gridLons=None, #To calculate statistical northness and eastness\n",
    "    output_path=None\n",
    "):\n",
    "     \"\"\"\n",
    "    Process bottom layer data for a specified variable in a NetCDF file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the NetCDF file.\n",
    "    - variable_name (str): Name of the variable to process.\n",
    "    - output_path (str): Path to save the processed file (optional). If None, the result is not saved.\n",
    "    \n",
    "    Returns:\n",
    "    - xarray.DataArray: The time-averaged bottom layer data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if output path is valid\n",
    "    if output_path is not None:\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        elif not os.access(output_dir, os.W_OK):\n",
    "            raise PermissionError(f\"Write permission denied for the directory: {output_dir}\")\n",
    "\n",
    "    time_start = time.time()\n",
    "\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    print(f\"\\nAccessed the dataset after {time.time() - time_start:.2f} seconds\")\n",
    "\n",
    "   for dim in [\"time\", \"xc\", \"yc\"]:\n",
    "        if dim in ds.dims:  \n",
    "            ds = ds.dropna(dim=dim, how=\"any\")\n",
    "\n",
    "    #Extract the  desired variable\n",
    "    if variable_name == \"current_speed\":\n",
    "        data_var = ds[\"u_velocity\"]\n",
    "\n",
    "    elif variable_name == \"statistical_northness\" or variable_name == \"statistical_eastness\":\n",
    "        data_var = ds[\"u_velocity\"]\n",
    "\n",
    "    else:\n",
    "        data_var = ds[variable_name]\n",
    "    \n",
    "    #Extract the first time step\n",
    "    time_slice = data_var.isel(time=0)\n",
    "    \n",
    "    #Create a mask for valid values in first time step\n",
    "    valid_mask = ~time_slice.isnull()\n",
    "    \n",
    "    #Find the index of the bottom-most valid layer for each (yc, xc)\n",
    "    #Subtract 1 to get the correct index for the bottom layer\n",
    "    bottom_layer_idx = valid_mask.argmin(dim=\"zc\") - 1\n",
    "\n",
    "    #Ensure bottom_layer_idx does not go negative (e.g., if all values are invalid in a column)\n",
    "    bottom_layer_idx = bottom_layer_idx.clip(min=0)\n",
    "\n",
    "    #Extract the bottom layer data across all time steps\n",
    "    if variable_name == \"current_speed\":\n",
    "        bottom_layer_data = (data_var.isel(zc=bottom_layer_idx)**2 + ds[\"v_velocity\"].isel(zc=bottom_layer_idx)**2)**0.5\n",
    "        \n",
    "    elif variable_name == \"statistical_northness\" or variable_name == \"statistical_eastness\":\n",
    "        longitude_of_projection_origin = ds[\"grid_mapping\"].attrs[\"longitude_of_projection_origin\"]\n",
    "        theta = gridLons - longitude_of_projection_origin\n",
    "        eastward_velocity = data_var.isel(zc=bottom_layer_idx)* np.cos(np.deg2rad(theta)) - ds[\"v_velocity\"].isel(zc=bottom_layer_idx)*np.sin(np.deg2rad(theta))\n",
    "        northward_velocity = data_var.isel(zc=bottom_layer_idx)* np.sin(np.deg2rad(theta)) + ds[\"v_velocity\"].isel(zc=bottom_layer_idx)* np.cos(np.deg2rad(theta))\n",
    "        aspect = np.arctan2(eastward_velocity, northward_velocity)\n",
    "\n",
    "        if variable_name == 'statistical_eastness':\n",
    "            bottom_layer_data = np.sin(aspect)\n",
    "        else:\n",
    "            bottom_layer_data = np.cos(aspect)\n",
    "        \n",
    "    else:\n",
    "        bottom_layer_data = data_var.isel(zc=bottom_layer_idx)\n",
    "        \n",
    "    ds.close()\n",
    "\n",
    "    print(f\"\\nExtracted the bottom layer data after {time.time() - time_start:.2f} seconds.\\n\\nStarting computation of statistics...\")\n",
    "\n",
    "    if variable_name == 'temperature_sundahl':\n",
    "        #Calculate Min(mean March-May) and Max(mean Oct-Dec)\n",
    "\n",
    "        min_march_may = #\n",
    "        max_oct_dec = #\n",
    "        print(f\"\\nComputed statistics after {time.time() - time_start:.2f} seconds\")\n",
    "\n",
    "        #Create a new DataArray with the (min, max) and explicitly define the 'stat' dimension\n",
    "        #Concatenate \n",
    "        stats_array = xr.concat([min_march_may, max_oct_dec], dim=\"stat\").rename(f\"{variable_name}_features\")\n",
    "        stats_array = stats_array.assign_coords(stat=[\"min_(mean_march_may)\", \"max_(mean_oct_dec)\"])\n",
    "\n",
    "\n",
    "    else:\n",
    "        #Calculate statistics across time\n",
    "        time_avg_bottom_layer = bottom_layer_data.mean(dim=\"time\", skipna=True)\n",
    "\n",
    "        #Calculate both 10th and 90th percentiles\n",
    "        time_percentiles = bottom_layer_data.quantile([0.1, 0.9], dim=\"time\", skipna=True)\n",
    "    \n",
    "        print(f\"\\nComputed statistics after {time.time() - time_start:.2f} seconds\")\n",
    "    \n",
    "        #Create a new DataArray with the (mean, 10th, 90th) percentiles and explicitly define the 'stat' dimension\n",
    "        #Concatenate mean and percentiles in one line, drop 'quantile' and concatenate all together\n",
    "        stats_array = xr.concat([time_avg_bottom_layer, time_percentiles.sel(quantile=0.1).drop_vars(\"quantile\"), time_percentiles.sel(quantile=0.9).drop_vars(\"quantile\")], dim=\"stat\").rename(f\"{variable_name}_features\")\n",
    "        stats_array = stats_array.assign_coords(stat=[\"mean\", \"10th_percentile\", \"90th_percentile\"])\n",
    "\n",
    "    #Save to output file if specified\n",
    "    if output_path:\n",
    "        stats_array.to_netcdf(output_path, mode='w')\n",
    "\n",
    "    return stats_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f844939f-b80b-4ced-b866-954f59644623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run on temperature data\n",
    "#process_bottom_layer_no_dask(\"/cluster/projects/itk-SINMOD/coral-mapping/midnor/PhysStates_2019.nc\", \"temperature\", output_path=\"/cluster/home/haroldh/coral-mapping/processed_data/features/temperature_bottom_features.nc\")\n",
    "\n",
    "#process_bottom_layer_no_dask(\"/cluster/projects/itk-SINMOD/coral-mapping/midnor/PhysStates_2019.nc\", \"salinity\", output_path=\"/cluster/home/haroldh/coral-mapping/processed_data/features/salinity_bottom_features.nc\")\n",
    "\n",
    "#process_bottom_layer_no_dask(\"/cluster/projects/itk-SINMOD/coral-mapping/midnor/PhysStates_2019.nc\", \"current_speed\", output_path=\"/cluster/home/haroldh/coral-mapping/processed_data/features/current_speed_bottom_features.nc\")\n",
    "\n",
    "#For Sundahl's definiton of temperature\n",
    "#process_bottom_layer_no_dask(\"filepath\", \"temperature_sundahl\", output_path=\"\")\n",
    "\n",
    "\n",
    "#Run on statistical northness\n",
    "#process_bottom_layer_no_dask(\"/cluster/projects/itk-SINMOD/coral-mapping/midnor/PhysStates_2019.nc\", \"statistical_northness\", gridLons, output_path=\"/cluster/home/haroldh/coral-mapping/processed_data/features/statistical_northness_features.nc\")\n",
    "\n",
    "# process_bottom_layer_no_dask(\"/cluster/projects/itk-SINMOD/coral-mapping/midnor/PhysStates_2019.nc\", \"statistical_eastness\", gridLons, output_path=\"/cluster/home/haroldh/coral-mapping/processed_data/features/statistical_eastness_features.nc\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
