{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Load the 4D SINMOD dataset\n",
    "ds = xr.open_dataset(\"/cluster/projects/itk-SINMOD/coral-mapping/midnor/PhysStates_2019.nc\")\n",
    "\n",
    "#Load the 2D midnor SINMOD dataset to get \"gridLons\" variable\n",
    "ds_2d = xr.open_dataset(\"/cluster/projects/itk-SINMOD/coral-mapping/midnor/samp_2D_jan_jun.nc\")\n",
    "gridLons = ds_2d[\"gridLons\"]\n",
    "\n",
    "# Check the dataset structure\n",
    "#print(ds)\n",
    "\n",
    "#Add gridLons to ds\n",
    "ds['gridLons'] = gridLons\n",
    "\n",
    "# Check the result\n",
    "print(ds)\n",
    "\n",
    "# List of variables for processing\n",
    "variables = ['temperature', 'salinity', 'u_velocity', 'v_velocity']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import dask\n",
    "import time \n",
    "\n",
    "def process_bottom_layer(\n",
    "    file_path,\n",
    "    variable_name,\n",
    "    chunks={\"time\":-1, \"zc\": -1, \"yc\": 50, \"xc\": 50},\n",
    "    output_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Process bottom layer data for a specified variable in a NetCDF file.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path (str): Path to the NetCDF file.\n",
    "    - variable_name (str): Name of the variable to process.\n",
    "    - chunks (dict): Chunking strategy for the dataset.\n",
    "    - slice_dict (dict): Slicing parameters for the data (e.g., {\"zc\": slice(0, 10), \"yc\": slice(0, 10), \"xc\": slice(65, 75)}).\n",
    "    - output_path (str): Path to save the processed file (optional). If None, the result is not saved.\n",
    "    \n",
    "    Returns:\n",
    "    - xarray.DataArray: The time-averaged bottom layer data.\n",
    "    \"\"\"\n",
    "    time_start = time.time()\n",
    "\n",
    "    # Open the dataset with chunking\n",
    "    ds = xr.open_dataset(file_path, chunks=chunks)\n",
    "\n",
    "    print(f\"\\nAccessed the dataset in {time.time() - time_start:.2f} seconds\")\n",
    "    \n",
    "    # Extract the variable\n",
    "    if variable_name == \"current_speed\":\n",
    "        data_var = ds[\"u_velocity\"]\n",
    "    else:\n",
    "        data_var = ds[variable_name]\n",
    "    \n",
    "    # Extract the first time step\n",
    "    time_slice = data_var.isel(time=0)\n",
    "    \n",
    "    # Step 1: Create a mask for valid values in first time step\n",
    "    valid_mask = ~time_slice.isnull()\n",
    "    \n",
    "    # Step 2: Find the index of the bottom-most valid layer for each (yc, xc)\n",
    "    # Subtract 1 to get the correct index for the bottom layer\n",
    "    bottom_layer_idx = valid_mask.argmin(dim=\"zc\") - 1\n",
    "\n",
    "    # Ensure bottom_layer_idx does not go negative (e.g., if all values are invalid in a column)\n",
    "    bottom_layer_idx = bottom_layer_idx.clip(min=0)\n",
    "    \n",
    "    # Step 3: Extract the bottom layer data across all time steps\n",
    "    if variable_name == \"current_speed\":\n",
    "        bottom_layer_data = (data_var.isel(zc=bottom_layer_idx)**2 + ds[\"v_velocity\"].isel(zc=bottom_layer_idx)**2)**0.5\n",
    "    else:\n",
    "        bottom_layer_data = data_var.isel(zc=bottom_layer_idx.load())\n",
    "\n",
    "    print(f\"\\nExtracted the bottom layer data in {time.time() - time_start:.2f} seconds.\\n\\nStarting computation of statistics...\")\n",
    "\n",
    "    # CONSIDER CHUNKING THE BOTTOM LAYER DATA FOR EFFICIENT COMPUTATION\n",
    "    # bottom_layer_data = bottom_layer_data.chunk({\"yc\": 50, \"xc\": 50})\n",
    "\n",
    "    # Step 4: Calculate statistics across time\n",
    "    time_avg_bottom_layer = bottom_layer_data.mean(dim=\"time\", skipna=True)\n",
    "\n",
    "    # Calculate both 10th and 90th percentiles\n",
    "    time_percentiles = bottom_layer_data.quantile([0.1, 0.9], dim=\"time\", skipna=True)\n",
    "\n",
    "    print(f\"\\nComputed statistics after {time.time() - time_start:.2f} seconds\")\n",
    "\n",
    "    # Create a new DataArray with the (mean, 10th, 90th) percentiles and explicitly define the 'stat' dimension\n",
    "    # Concatenate mean and percentiles in one line, drop 'quantile' and concatenate all together\n",
    "    stats_array = xr.concat([time_avg_bottom_layer, time_percentiles.sel(quantile=0.1).drop_vars(\"quantile\"), time_percentiles.sel(quantile=0.9).drop_vars(\"quantile\")], dim=\"stat\").rename(f\"{variable_name}_features\")\n",
    "\n",
    "    # Name each value of the first dimension\n",
    "    stats_array = stats_array.assign_coords(stat=[\"mean\", \"10th_percentile\", \"90th_percentile\"])\n",
    "    # Save to output file if specified\n",
    "    if output_path:\n",
    "        stats_array.to_netcdf(output_path)\n",
    "    \n",
    "    return stats_array, bottom_layer_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accessed the dataset in 0.02 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2420823/1360436654.py:27: UserWarning: The specified chunks separate the stored chunks along dimension \"yc\" starting at index 50. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(file_path, chunks=chunks)\n",
      "/tmp/ipykernel_2420823/1360436654.py:27: UserWarning: The specified chunks separate the stored chunks along dimension \"xc\" starting at index 50. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  ds = xr.open_dataset(file_path, chunks=chunks)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Vectorized indexing with Dask arrays is not supported. Please pass a numpy array by calling ``.compute``. See https://github.com/dask/dask/issues/8958.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m speed_array, idx \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_bottom_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/cluster/projects/itk-SINMOD/coral-mapping/midnor/PhysStates_2019.nc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcurrent_speed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m speed_array\n",
      "Cell \u001b[0;32mIn[5], line 52\u001b[0m, in \u001b[0;36mprocess_bottom_layer\u001b[0;34m(file_path, variable_name, chunks, output_path)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Step 3: Extract the bottom layer data across all time steps\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m variable_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_speed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 52\u001b[0m     bottom_layer_data \u001b[38;5;241m=\u001b[39m (\u001b[43mdata_var\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbottom_layer_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m ds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv_velocity\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misel(zc\u001b[38;5;241m=\u001b[39mbottom_layer_idx)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     bottom_layer_data \u001b[38;5;241m=\u001b[39m data_var\u001b[38;5;241m.\u001b[39misel(zc\u001b[38;5;241m=\u001b[39mbottom_layer_idx\u001b[38;5;241m.\u001b[39mload())\n",
      "File \u001b[0;32m~/PyEnvCoralMapping/lib/python3.12/site-packages/xarray/core/dataarray.py:1540\u001b[0m, in \u001b[0;36mDataArray.isel\u001b[0;34m(self, indexers, drop, missing_dims, **indexers_kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m indexers \u001b[38;5;241m=\u001b[39m either_dict_or_kwargs(indexers, indexers_kwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(is_fancy_indexer(idx) \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indexers\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m-> 1540\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_temp_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_isel_fancy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_dims\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_temp_dataset(ds)\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;66;03m# Much faster algorithm for when all indexers are ints, slices, one-dimensional\u001b[39;00m\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;66;03m# lists, or zero or one-dimensional np.ndarray's\u001b[39;00m\n",
      "File \u001b[0;32m~/PyEnvCoralMapping/lib/python3.12/site-packages/xarray/core/dataset.py:3152\u001b[0m, in \u001b[0;36mDataset._isel_fancy\u001b[0;34m(self, indexers, drop, missing_dims)\u001b[0m\n\u001b[1;32m   3148\u001b[0m var_indexers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3149\u001b[0m     k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m valid_indexers\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m var\u001b[38;5;241m.\u001b[39mdims\n\u001b[1;32m   3150\u001b[0m }\n\u001b[1;32m   3151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m var_indexers:\n\u001b[0;32m-> 3152\u001b[0m     new_var \u001b[38;5;241m=\u001b[39m \u001b[43mvar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvar_indexers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3153\u001b[0m     \u001b[38;5;66;03m# drop scalar coordinates\u001b[39;00m\n\u001b[1;32m   3154\u001b[0m     \u001b[38;5;66;03m# https://github.com/pydata/xarray/issues/6554\u001b[39;00m\n\u001b[1;32m   3155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoords \u001b[38;5;129;01mand\u001b[39;00m drop \u001b[38;5;129;01mand\u001b[39;00m new_var\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/PyEnvCoralMapping/lib/python3.12/site-packages/xarray/core/variable.py:1079\u001b[0m, in \u001b[0;36mVariable.isel\u001b[0;34m(self, indexers, missing_dims, **indexers_kwargs)\u001b[0m\n\u001b[1;32m   1076\u001b[0m indexers \u001b[38;5;241m=\u001b[39m drop_dims_from_indexers(indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims, missing_dims)\n\u001b[1;32m   1078\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(indexers\u001b[38;5;241m.\u001b[39mget(dim, \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims)\n\u001b[0;32m-> 1079\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/PyEnvCoralMapping/lib/python3.12/site-packages/xarray/core/variable.py:825\u001b[0m, in \u001b[0;36mVariable.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m    813\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a new Variable object whose contents are consistent with\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;124;03m    getting the provided key from the underlying data.\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;124;03m    array `x.values` directly.\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 825\u001b[0m     dims, indexer, new_order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_broadcast_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m     indexable \u001b[38;5;241m=\u001b[39m as_indexable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data)\n\u001b[1;32m    828\u001b[0m     data \u001b[38;5;241m=\u001b[39m indexing\u001b[38;5;241m.\u001b[39mapply_indexer(indexable, indexer)\n",
      "File \u001b[0;32m~/PyEnvCoralMapping/lib/python3.12/site-packages/xarray/core/variable.py:676\u001b[0m, in \u001b[0;36mVariable._broadcast_indexes\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, Variable):\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(k\u001b[38;5;241m.\u001b[39mdims) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 676\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_broadcast_indexes_vectorized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    677\u001b[0m     dims\u001b[38;5;241m.\u001b[39mappend(k\u001b[38;5;241m.\u001b[39mdims[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, integer_types):\n",
      "File \u001b[0;32m~/PyEnvCoralMapping/lib/python3.12/site-packages/xarray/core/variable.py:810\u001b[0m, in \u001b[0;36mVariable._broadcast_indexes_vectorized\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    808\u001b[0m     new_order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_dims, \u001b[43mVectorizedIndexer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout_key\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, new_order\n",
      "File \u001b[0;32m~/PyEnvCoralMapping/lib/python3.12/site-packages/xarray/core/indexing.py:476\u001b[0m, in \u001b[0;36mVectorizedIndexer.__init__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    474\u001b[0m     k \u001b[38;5;241m=\u001b[39m as_integer_slice(k)\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_duck_dask_array(k):\n\u001b[0;32m--> 476\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVectorized indexing with Dask arrays is not supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease pass a numpy array by calling ``.compute``. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/dask/dask/issues/8958.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    480\u001b[0m     )\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_duck_array(k):\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(k\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger):\n",
      "\u001b[0;31mValueError\u001b[0m: Vectorized indexing with Dask arrays is not supported. Please pass a numpy array by calling ``.compute``. See https://github.com/dask/dask/issues/8958."
     ]
    }
   ],
   "source": [
    "speed_array, idx = process_bottom_layer(\"/cluster/projects/itk-SINMOD/coral-mapping/midnor/PhysStates_2019.nc\", \"current_speed\")\n",
    "speed_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import time \n",
    "\n",
    "def process_bottom_layer_no_dask(\n",
    "    file_path,\n",
    "    variable_name,\n",
    "    #chunks={\"time\":-1, \"zc\": -1, \"yc\": 50, \"xc\": 50},\n",
    "    output_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Process bottom layer data for a specified variable in a NetCDF file.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path (str): Path to the NetCDF file.\n",
    "    - variable_name (str): Name of the variable to process.\n",
    "    - chunks (dict): Chunking strategy for the dataset.\n",
    "    - slice_dict (dict): Slicing parameters for the data (e.g., {\"zc\": slice(0, 10), \"yc\": slice(0, 10), \"xc\": slice(65, 75)}).\n",
    "    - output_path (str): Path to save the processed file (optional). If None, the result is not saved.\n",
    "    \n",
    "    Returns:\n",
    "    - xarray.DataArray: The time-averaged bottom layer data.\n",
    "    \"\"\"\n",
    "    time_start = time.time()\n",
    "\n",
    "    # Open the dataset\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    print(f\"\\nAccessed the dataset after {time.time() - time_start:.2f} seconds\")\n",
    "    \n",
    "    # Extract the variable\n",
    "    if variable_name == \"current_speed\":\n",
    "        data_var = ds[\"u_velocity\"][:,:,0:10,65:75]\n",
    "    else:\n",
    "        data_var = ds[variable_name]\n",
    "    \n",
    "    # Extract the first time step\n",
    "    time_slice = data_var.isel(time=0)\n",
    "    \n",
    "    # Step 1: Create a mask for valid values in first time step\n",
    "    valid_mask = ~time_slice.isnull()\n",
    "    \n",
    "    # Step 2: Find the index of the bottom-most valid layer for each (yc, xc)\n",
    "    # Subtract 1 to get the correct index for the bottom layer\n",
    "    bottom_layer_idx = valid_mask.argmin(dim=\"zc\") - 1\n",
    "\n",
    "    # Ensure bottom_layer_idx does not go negative (e.g., if all values are invalid in a column)\n",
    "    bottom_layer_idx = bottom_layer_idx.clip(min=0)\n",
    "    \n",
    "    # Step 3: Extract the bottom layer data across all time steps\n",
    "    if variable_name == \"current_speed\":\n",
    "        bottom_layer_data = (data_var.isel(zc=bottom_layer_idx)**2 + ds[\"v_velocity\"][:,:,:10,65:75].isel(zc=bottom_layer_idx)**2)**0.5\n",
    "    else:\n",
    "        bottom_layer_data = data_var.isel(zc=bottom_layer_idx)\n",
    "\n",
    "    ds.close()\n",
    "\n",
    "    print(f\"\\nExtracted the bottom layer data after {time.time() - time_start:.2f} seconds.\\n\\nStarting computation of statistics...\")\n",
    "\n",
    "    # Step 4: Calculate statistics across time\n",
    "    time_avg_bottom_layer = bottom_layer_data.mean(dim=\"time\", skipna=True)\n",
    "\n",
    "    # Calculate both 10th and 90th percentiles\n",
    "    time_percentiles = bottom_layer_data.quantile([0.1, 0.9], dim=\"time\", skipna=True)\n",
    "\n",
    "    print(f\"\\nComputed statistics after {time.time() - time_start:.2f} seconds\")\n",
    "\n",
    "    # Create a new DataArray with the (mean, 10th, 90th) percentiles and explicitly define the 'stat' dimension\n",
    "    # Concatenate mean and percentiles in one line, drop 'quantile' and concatenate all together\n",
    "    stats_array = xr.concat([time_avg_bottom_layer, time_percentiles.sel(quantile=0.1).drop_vars(\"quantile\"), time_percentiles.sel(quantile=0.9).drop_vars(\"quantile\")], dim=\"stat\").rename(f\"{variable_name}_features\")\n",
    "\n",
    "    # Name each value of the first dimension\n",
    "    stats_array = stats_array.assign_coords(stat=[\"mean\", \"10th_percentile\", \"90th_percentile\"])\n",
    "\n",
    "    # Save to output file if specified\n",
    "    if output_path:\n",
    "        stats_array.to_netcdf(output_path)\n",
    "    \n",
    "    return stats_array, bottom_layer_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accessed the dataset after 0.18 seconds\n"
     ]
    }
   ],
   "source": [
    "current_array_loaded, idx_no_dask = process_bottom_layer_no_dask(\"/cluster/projects/itk-SINMOD/coral-mapping/midnor/PhysStates_2019.nc\", \"temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(current_array_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "variable_name = \"current_speed\"\n",
    "data_array = current_array_loaded\n",
    "\n",
    "# Visualize the results for the entire data\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot mean, 10th percentile, and 90th percentile for the entire dataset\n",
    "data_array.sel(stat=\"mean\").plot(ax=axes[0])\n",
    "axes[0].set_title(f'{variable_name} - Mean')\n",
    "\n",
    "data_array.sel(stat=\"10th_percentile\").plot(ax=axes[1])\n",
    "axes[1].set_title(f'{variable_name} - 10th Percentile')\n",
    "\n",
    "data_array.sel(stat=\"90th_percentile\").plot(ax=axes[2])\n",
    "axes[2].set_title(f'{variable_name} - 90th Percentile')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'temp_array_no_dask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m data_slice \u001b[38;5;241m=\u001b[39m \u001b[43mtemp_array_no_dask\u001b[49m\u001b[38;5;241m.\u001b[39misel(xc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m65\u001b[39m, \u001b[38;5;241m75\u001b[39m), yc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 1. Assert that if one of the statistics is NaN, then all three (mean, 10th, 90th percentiles) should also be NaN\u001b[39;00m\n\u001b[1;32m      6\u001b[0m nan_mask \u001b[38;5;241m=\u001b[39m data_slice\u001b[38;5;241m.\u001b[39msel(stat\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misnull() \u001b[38;5;241m|\u001b[39m data_slice\u001b[38;5;241m.\u001b[39msel(stat\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10th_percentile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misnull() \u001b[38;5;241m|\u001b[39m data_slice\u001b[38;5;241m.\u001b[39msel(stat\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m90th_percentile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misnull()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'temp_array_no_dask' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_slice = temp_array_no_dask.isel(xc=slice(65, 75), yc=slice(0, 10))\n",
    "\n",
    "# 1. Assert that if one of the statistics is NaN, then all three (mean, 10th, 90th percentiles) should also be NaN\n",
    "nan_mask = data_slice.sel(stat=\"mean\").isnull() | data_slice.sel(stat=\"10th_percentile\").isnull() | data_slice.sel(stat=\"90th_percentile\").isnull()\n",
    "assert (nan_mask == (data_slice.sel(stat=\"mean\").isnull() & data_slice.sel(stat=\"10th_percentile\").isnull() & data_slice.sel(stat=\"90th_percentile\").isnull())).all(), \"If one of the stats is NaN, all of them should be NaN for that grid point.\"\n",
    "\n",
    "# 2. Assert that the mean should be greater than or equal to the 10th percentile, and less than or equal to the 90th percentile, ignoring NaNs\n",
    "assert (data_slice.sel(stat=\"mean\").notnull() >= data_slice.sel(stat=\"10th_percentile\").notnull()).all(), \"Mean should be greater than or equal to the 10th percentile\"\n",
    "assert (data_slice.sel(stat=\"mean\").notnull() <= data_slice.sel(stat=\"90th_percentile\").notnull()).all(), \"Mean should be less than or equal to the 90th percentile\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_slice.where(~nan_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
